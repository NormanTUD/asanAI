Fehlt: Cathy O’Neil (2016): Weapons of Math Destruction. Über die soziale Auswirkung von Algorithmen.

Fehlt: Kate Crawford: Atlas of AI. Sie thematisiert die physischen Ressourcen (Lithium, Arbeit in Billiglohnländern), was einen guten Kontrast zur "abstrakten" Philosophie bildet.

Fehlt: Schmidhuber (1997): Du hast Hochreiter (1991) für die Diplomarbeit, aber das Paper zu LSTM (Long Short-Term Memory) von 1997 ist der eigentliche technische Standard, der die Spracherkennung für 20 Jahre dominierte.

Fehlt: The Bitter Lesson (Rich Sutton): Hast du drin (sehr gut!), aber vielleicht ergänzt durch Brooks (1990): Elephants Don't Play Chess. Das war die Abkehr von der hohen Logik hin zur Robotik/Embodiment.


https://nlp.seas.harvard.edu/2018/04/03/attention.html


Eliza

http://neuralnetworksanddeeplearning.com/chap4.html

https://en.wikipedia.org/wiki/Mark_I_Perceptron

Radford, A., et al. (OpenAI, 2018/2019): Improving Language Understanding by Generative Pre-training (Das erste GPT-Paper).

Buolamwini, J., & Gebru, T. (2018): Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.

Kingma, D. P., & Ba, J. (2014): Adam: A Method for Stochastic Optimization.

"If A and B have almost identical environments we say that they are synonyms"

Dataset Splits

Bias in Data

Vanishing/Exploding Gradients: You mention that Layer Normalization helps gradients flow, but from a technical POV, it would be useful to explain why gradients tend to disappear or explode in deep networks without these techniques.

Inference vs. Training: You mention training processes, but you don't explicitly distinguish the technical difference in how the model operates during Inference (generating text) versus Training (updating weights).

Add a "Tiny-Implementation": A few lines of code or a logic flow for a minimal system.

Address Hardware/Scaling: Explain that "Intelligence" in these systems is often a result of scaling these simple concepts across billions of parameters and thousands of GPUs.

1. The "Mathematical Bridge" (Linear Algebra)
You have "1+1=2" and "Derivatives," but you are missing the middle step: Matrices.
    The Gap: You mention vectors in your embedding section, but you haven't explained that a Transformer is essentially just a series of massive matrix multiplications (W⋅x+b).
    Why it's needed: A beginner won't understand how a "Minimal Neuron" scales up to a "Transformer" without understanding that we don't calculate one neuron at a time; we calculate thousands at once using grids of numbers.
    Missing Concept: Matrix Multiplication as "Parallel Processing."

3. The "Token Lifecycle" (Connecting the Modules)
You have sections on Tokenizers, Embeddings, Attention, and Softmax. However, these are currently "islands" of knowledge.
    The Gap: You need a Data Flow Narrative.
    The Solution: A chapter called "The Life of a Prompt." Trace the word "Hello" through every single component you've written about.
        Step 1: Tokenizer turns it into ID 15496.
        Step 2: Embedding turns it into a vector.
        Step 3: Attention looks at the words around it.
        Step 4: Softmax predicts the next word "world."
    Why it's needed: This turns a list of definitions into a functional machine in the reader's mind.

https://www.anthropic.com/news/golden-gate-claude

2. Word Embeddings (1957)
"You shall know a word by the company it keeps." — John Rupert Firth, A Synopsis of Linguistic Theory (Note: This linguistic philosophy is the direct ancestor of "Word2Vec" and how LLMs understand meaning through context.)

3. Backpropagation (1986)
"We describe a new learning procedure, back-propagation, for networks of neuron-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector." — Rumelhart, Hinton, & Williams, Learning representations by back-propagating errors


4. The Vanishing Gradient Problem (1991)
"The error signal used to train the network tends to either explode or vanish as it is propagated back through time... this makes it difficult for a network to learn to bridge long time lags." — Sepp Hochreiter, Untersuchungen zu dynamischen neuronalen Netzen (Note: This problem led to the invention of LSTMs and, eventually, the Transformer.)

write more about data
5. ImageNet and the Data Shift (2009)
"To date, the field of computer vision has been focused on algorithms... However, the complexity of the visual world is such that the bottleneck of progress is likely to be the lack of data." — Deng et al., ImageNet: A Large-Scale Hierarchical Image Database

6. Deep Learning’s Resurgence (2012)
"To fight overfitting, we use a recently-introduced technique called 'dropout' that consists of setting to zero the output of each hidden neuron with probability 0.5." — Krizhevsky, Sutskever, & Hinton, ImageNet Classification with Deep Convolutional Neural Networks

1. The Physical Basis of Mind (1943)
"Because of the 'all-or-none' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic." — McCulloch & Pitts, A Logical Calculus of the Ideas Immanent in Nervous Activity (Note: This was the first time anyone suggested that a biological neuron could be modeled as a mathematical function.)

4. Word2Vec: Semantic Arithmetic (2013)
"It is somewhat surprising that the learned word representations can be used to solve analogy questions via simple algebraic operations: Vector(’King’)−Vector(’Man’)+Vector(’Woman’) results in a vector that is closest to Vector(’Queen’)." — Tomas Mikolov et al., Efficient Estimation of Word Representations in Vector Space

5. Reinforcement Learning from Human Feedback (2017/2022)
"If we want to apply reinforcement learning to complex real-world tasks, we need a way to provide a reward signal that is aligned with human intentions... even if the human cannot specify a formal reward function." — Christiano et al., Deep Reinforcement Learning from Human Preferences (Note: This is the technology that "tames" raw LLMs into helpful assistants like ChatGPT.)

7. The Lottery Ticket Hypothesis (2019)
"A randomly-initialized, dense neural network contains a subnetwork that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations." — Frankle & Carbin, The Lottery Ticket Hypothesis (Note: This suggests that much of a massive AI model is "dead weight," and we are looking for a lucky "winning ticket" during training.)

10. The Hardware Lottery (2020)
"The success of a research idea is often determined by its compatibility with the available hardware and software ecosystem, rather than its inherent superiority." — Sara Hooker, The Hardware Lottery (Note: Deep learning won not just because it was 'better,' but because it ran perfectly on GPUs designed for video games.)

2. Distributed Representations (1952)
"" — Donald Hebb, The Organization of Behavior (Note: Often summarized as "Neurons that fire together, wire together," this is the biological inspiration for neural network weights.)

3. The Power of "Noisy" Data (2011)
"We should stop acting as if our goal is to author extremely elegant algorithms, and instead embrace the fact that for many problems, data is the bottleneck... Simple models on a lot of data trump complex models on a small amount of data." — Peter Norvig, The Unreasonable Effectiveness of Data

4. ResNet: Training Deep Networks (2015)
"When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated... We address this by introducing a deep residual learning framework." — He et al., Deep Residual Learning for Image Recognition (Note: This "Skip Connection" allows us to train networks with hundreds or thousands of layers.)

6. Self-Supervised Learning (2020)
"The next revolution in AI will not be supervised, nor purely reinforced. It will be self-supervised learning: systems that learn by predicting masked or missing parts of their input." — Yann LeCun, AAAI Invited Talk

7. Chain of Thought Prompting (2022)
"We explore how generating a series of intermediate reasoning steps—a chain of thought—significantly improves the ability of large language models to perform complex reasoning." — Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

9. Mixture of Experts (2017)
"The capacity of a neural network to absorb information is limited by its number of parameters... We introduce a Sparsely-Gated Mixture-of-Experts layer, which allows for a massive increase in model capacity with only a minor increase in computation." — Shazeer et al., Outrageously Large Neural Networks

1. The Linguistic Precursor (1954)
"The entire vocabulary is distributed in a multi-dimensional space... The distance between two words in this space is a measure of their semantic difference." — Zellig Harris, Distributional Structure (Note: Harris predicted the "Vector Space" that LLMs use today decades before computers could actually build one.)

2. The Nature of Next-Token Prediction (2023)
"An LLM is a statistical word guesser; it is trying to predict what the next words would be if the same sentence were to appear on the internet... These are not indications that it 'believes' one thing or the other." — Mark Riedl, A Very Gentle Introduction to LLMs
